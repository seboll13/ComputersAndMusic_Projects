{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e39f30",
   "metadata": {},
   "source": [
    "## The following code uses the Pythonspaudiopy package\n",
    "- Docs: https://spaudiopy.readthedocs.io/en/latest/index.html\n",
    "- GitHub: https://github.com/chris-hld/spaudiopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1fa1cb",
   "metadata": {},
   "source": [
    "## Tools\n",
    "We use three parameters to locate an HRTF:\n",
    "1. *Azimuth*: angle between position and sound location on the $xy$-plane\n",
    "2. *Elevation*: angle between position and sound location on the $xz$-plane\n",
    "3. *Time* or *Frequency*: time period or frequency of emitted sound w.r.t. actual position\n",
    "\n",
    "Math tools:\n",
    "- *Haversine distance*: Consider two points $x_1$ and $x_2$ on a sphere with respective latitudes and longitudes $(\\varphi_1,\\varphi_2)$ and $(\\theta_1,\\theta_2)$. The Haversine distance $D(x_1,x_2)$ is the angular distance between them on the surface of the sphere given by $$D(x_1,x_2)=2\\arcsin\\sqrt{\\sin^2\\left(\\frac{\\varphi_2-\\varphi_1}{2}\\right)+\\cos x_1\\cos y_1\\sin^2\\left(\\frac{\\theta_2-\\theta_1}{2}\\right)}.$$ We use this distance when wanting to find the closest HRIR point from a grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a5acb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check spaudiopy.sig functions to open MONO signal and play with HRIRs\n",
    "import spaudiopy as spa\n",
    "from spaudiopy.sig import MonoSignal as ms\n",
    "from spaudiopy.sig import MultiSignal as stereo \n",
    "import spaudiopy.process as sproc\n",
    "\n",
    "import scipy.signal\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "\n",
    "fs=44100 # sampling rate\n",
    "fs2=96000\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3434822",
   "metadata": {},
   "source": [
    "## Simple spatialisation with delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd29bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stereo_tracks(music_file):\n",
    "    return stereo.from_file(music_file).get_signals()\n",
    "\n",
    "def delay_signal(s, delay):\n",
    "    _len_sample = len(s)\n",
    "    d_arr = np.zeros(_len_sample, dtype=int)\n",
    "    d_arr[int(fs*delay)] = 1\n",
    "    return scipy.signal.convolve(s, d_arr)[:_len_sample]\n",
    "\n",
    "def spatialize_signal(s, delay):\n",
    "    _len_sample = len(s)\n",
    "    d_arr = np.zeros(_len_sample, dtype=int)\n",
    "    d_arr[0] = 1\n",
    "    d_arr[int(fs*delay)] = 1\n",
    "    return scipy.signal.convolve(s, d_arr)[:_len_sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4c054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MS_DELAY = 25 # delay in [ms] to create surrounding effect\n",
    "\n",
    "def spatialise_sound(music_file, left=False):\n",
    "    \"\"\"Create a spatialised version of the given input file\n",
    "        @param music_file: stereo music file\n",
    "        @param left: creates the delay to the left or the right given this value\n",
    "    \"\"\"\n",
    "    s1,s2 = get_stereo_tracks(music_file)\n",
    "    if left:\n",
    "        delayed_s1 = delay_signal(s1, MS_DELAY / 1000)\n",
    "        return stereo([delayed_s1, s2], fs=fs)\n",
    "    delayed_s2 = delay_signal(s2, MS_DELAY / 1000)\n",
    "    return stereo([s1, delayed_s2], fs=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7427139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatialise_sound_three(music_file, left=False):\n",
    "    \"\"\"Create a spatialised version of the given input file\n",
    "        @param music_file: stereo music file\n",
    "        @param left: creates the delay to the left or the right given this value\n",
    "    \"\"\"\n",
    "    s1,s2 = get_stereo_tracks(music_file)\n",
    "    if left:\n",
    "        delayed_s1 = delay_signal(s1, MS_DELAY / 1000)\n",
    "        return stereo([delayed_s1, s1, s2], fs=fs)\n",
    "    delayed_s2 = delay_signal(s2, MS_DELAY / 1000)\n",
    "    return stereo([s1, s2, delayed_s2], fs=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd61de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spatialise_sound(\"ocean_eyes.wav\")\n",
    "s.save(\"ocean_eyes_spatial_2tracks.wav\")\n",
    "display(Audio(s, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91109a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = spatialise_sound_three(\"ocean_eyes.wav\")\n",
    "s2.save(\"ocean_eyes_spatial_3tracks.wav\")\n",
    "display(Audio(s2, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5ec28",
   "metadata": {},
   "source": [
    "## Reverb for increased spatial effect\n",
    "Create simultaneous small delay effects and combine them together in a new signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a6f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: WORK IN PROGRESS\n",
    "def reverb(music_file):\n",
    "    s1,s2 = get_stereo_tracks(music_file)\n",
    "    tracks = [s1]\n",
    "    for _ in range(5):\n",
    "        delayed_s2 = delay_signal(s2, 5 / 1000)\n",
    "        tracks.append(delayed_s2)\n",
    "        s2 = delayed_s2\n",
    "    return stereo(tracks, fs=fs)\n",
    "\n",
    "s_rev = reverb(\"ocean_eyes.wav\")\n",
    "display(Audio(s_rev, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WORK IN PROGRESS\n",
    "def echo(s):\n",
    "    _filter = [1]\n",
    "    for i in range(1, len(s)):\n",
    "        _filter.append(_filter[-1]*0.5)\n",
    "    return _filter\n",
    "\n",
    "def successive_echo(s):\n",
    "    _filter = [1]\n",
    "    for i in range(1, len(s)):\n",
    "        if i % fs/2 == 0:\n",
    "            _filter.append(1)\n",
    "        else:\n",
    "            _filter.append(_filter[-1]*0.99)\n",
    "    return _filter\n",
    "\n",
    "def successive_delays(s, delay):\n",
    "    return scipy.signal.convolve(s, echo(s))[:len(s)]\n",
    "\n",
    "def reverb_v2(music_file):\n",
    "    s1,s2 = get_stereo_tracks(music_file)\n",
    "    delayed_s1 = delay_signal(s1, MS_DELAY / 1000)\n",
    "    delayed_s2 = successive_delays(s2, MS_DELAY / 1000)\n",
    "    return stereo([s1, s2, delayed_s2], fs=fs)\n",
    "\n",
    "s_rev2 = reverb_v2(\"ocean_eyes.wav\")\n",
    "display(Audio(s_rev2, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf5bb5",
   "metadata": {},
   "source": [
    "## Simple sound localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202591f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attenuate(sig, direction):\n",
    "    return [e*np.cos(direction/4) for e in sig]\n",
    "\n",
    "def localize_sound(stereo_music_file, direction, left=True):\n",
    "    sample = stereo.from_file(stereo_music_file)\n",
    "    s1, s2 = sample.get_signals()\n",
    "    sample_song_mono = fusion([s1,s2])\n",
    "    \n",
    "    maxmono = max(sample_song_mono)\n",
    "    mono_norm = []\n",
    "    for i in range(len(sample_song_mono)):\n",
    "        mono_norm.append(sample_song_mono[i]/(maxmono+1))\n",
    "        \n",
    "    delay = np.zeros(len(mono_norm), dtype=int)\n",
    "    delay[int(fs*compute_delay_from_angle(direction))] = 1\n",
    "    delayed = scipy.signal.convolve(mono_norm, delay)[:len(mono_norm)]\n",
    "    \n",
    "    if left:\n",
    "        return stereo([mono_norm, attenuate(delayed, direction)], fs=44100) # sound to the left\n",
    "    return stereo([attenuate(delayed, direction), mono_norm], fs=44100) # sound to the right\n",
    "\n",
    "display(Audio(localize_sound(\"ocean_eyes.wav\", np.pi/4), rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec225e4",
   "metadata": {},
   "source": [
    "## Pyroomacoustics tests for localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f171d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyroomacoustics as pra\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0779f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#signal = ms.from_file(\"groove.wav\").signal\n",
    "fs, signal = wavfile.read(\"groove.wav\")\n",
    "\n",
    "# Sound source part\n",
    "room = pra.ShoeBox([20,20])\n",
    "room.add_source([10, 10], signal=signal)\n",
    "\n",
    "# Microphones part\n",
    "#R = pra.circular_2D_array(center=[2.,2.], M=6, phi0=0, radius=0.1)\n",
    "#room.add_microphone_array(pra.MicrophoneArray(R, room.fs))\n",
    "R = np.array([[9, 11], [8, 8]])  # [[x], [y]]\n",
    "room.add_microphone(R)\n",
    "\n",
    "room.simulate()\n",
    "fig, ax = room.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306425f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatialise_signals(s1, s2):\n",
    "    delayed_s2 = delay_signal(s2, MS_DELAY / 1000)\n",
    "    return stereo([s1, delayed_s2], fs=fs)\n",
    "\n",
    "#snd = spatialise_signals(room.mic_array.signals[0,:], room.mic_array.signals[1,:])\n",
    "snd = stereo([room.mic_array.signals[0,:], room.mic_array.signals[1,:]], fs=fs)\n",
    "display(Audio(snd, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e162e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "song = stereo.from_file(\"Police_Roxanne/The-Police-Roxanne.wav\")\n",
    "bass = stereo.from_file(\"Police_Roxanne/bass.wav\")\n",
    "drums = stereo.from_file(\"Police_Roxanne/drums.wav\")\n",
    "other = stereo.from_file(\"Police_Roxanne/other.wav\")\n",
    "vocals = stereo.from_file(\"Police_Roxanne/vocals.wav\")\n",
    "\n",
    "song1, song2 = song.get_signals()\n",
    "b1, b2 = bass.get_signals()\n",
    "d1, d2 = drums.get_signals()\n",
    "o1, o2 = other.get_signals()\n",
    "v1, v2 = vocals.get_signals()\n",
    "\n",
    "mono_song = fusion([trim(song1), trim(song2)])\n",
    "mono_bass = fusion([trim(b1), trim(b2)])\n",
    "mono_drums = fusion([trim(d1), trim(d2)])\n",
    "mono_other = fusion([trim(o1), trim(o2)])\n",
    "mono_vocals = fusion([trim(v1), trim(v2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sound source part\n",
    "room = pra.ShoeBox([20,20])\n",
    "#room.add_source([10, 10], signal=mono_song)\n",
    "room.add_source([17, 8], signal=mono_bass)\n",
    "room.add_source([10, 17], signal=mono_drums)\n",
    "room.add_source([3, 8], signal=mono_other)\n",
    "#room.add_source([10, 12], signal=mono_vocals)\n",
    "\n",
    "# Microphones part\n",
    "#R = pra.circular_2D_array(center=[2.,2.], M=6, phi0=0, radius=0.1)\n",
    "#room.add_microphone_array(pra.MicrophoneArray(R, room.fs))\n",
    "R = np.array([[9, 11], [8, 8]])  # [[x], [y]]\n",
    "room.add_microphone(R)\n",
    "\n",
    "room.simulate()\n",
    "fig, ax = room.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shut_the_fuck_up(sound):\n",
    "    return [e*0.2 for e in sound]\n",
    "\n",
    "left = fusion([trim(room.mic_array.signals[0,:]), shut_the_fuck_up(spatialize(mono_vocals))])\n",
    "right = fusion([trim(room.mic_array.signals[1,:]), shut_the_fuck_up(mono_vocals)])\n",
    "\n",
    "#final = stereo([room.mic_array.signals[0,:], room.mic_array.signals[1,:]], fs=fs)\n",
    "final = stereo([left, right], fs=fs)\n",
    "display(Audio(final, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010c2bb",
   "metadata": {},
   "source": [
    "## Localise instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582128f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ear_distance = 0.21 # 21cm\n",
    "speed_of_sound = 343 # in m/s\n",
    "\n",
    "def trim(sig):\n",
    "    return sig[:44100*30]\n",
    "\n",
    "def compute_delay_from_angle(rad):\n",
    "    return avg_ear_distance * np.sin(rad) / speed_of_sound\n",
    "\n",
    "def fusion(signals):\n",
    "    s = []\n",
    "    mean = np.mean(signals, axis=0)\n",
    "    for i in range(len(signals[0])):\n",
    "        s.append(np.mean(mean[i]))\n",
    "    return s\n",
    "\n",
    "def delay(sig, direction):\n",
    "    delay = np.zeros(len(sig), dtype=int)\n",
    "    delay[int(fs*compute_delay_from_angle(direction))] = 1\n",
    "    \n",
    "    return scipy.signal.convolve(sig, delay)[:len(sig)]\n",
    "\n",
    "def spatialize(sig):\n",
    "    delay = np.zeros(len(sig), dtype=int)\n",
    "    delay[int(fs*0.025)] = 1\n",
    "    return scipy.signal.convolve(sig, delay)[:len(sig)]\n",
    "\n",
    "\n",
    "song = stereo.from_file(\"Police_Roxanne/The-Police-Roxanne.wav\")\n",
    "bass = stereo.from_file(\"Police_Roxanne/bass.wav\")\n",
    "drums = stereo.from_file(\"Police_Roxanne/drums.wav\")\n",
    "other = stereo.from_file(\"Police_Roxanne/other.wav\")\n",
    "vocals = stereo.from_file(\"Police_Roxanne/vocals.wav\")\n",
    "\n",
    "song1, song2 = song.get_signals()\n",
    "b1, b2 = bass.get_signals()\n",
    "d1, d2 = drums.get_signals()\n",
    "o1, o2 = other.get_signals()\n",
    "v1, v2 = vocals.get_signals()\n",
    "\n",
    "mono_song = fusion([trim(song1), trim(song2)])\n",
    "mono_bass = fusion([trim(b1), trim(b2)])\n",
    "mono_drums = fusion([trim(d1), trim(d2)])\n",
    "mono_other = fusion([trim(o1), trim(o2)])\n",
    "mono_vocals = fusion([trim(v1), trim(v2)])\n",
    "\n",
    "left = trim(fusion([mono_song, mono_bass, mono_drums, delay(mono_other, np.pi/2), spatialize(mono_vocals)]))\n",
    "right = trim(fusion([mono_song, mono_bass, delay(mono_drums, np.pi/2), mono_other, mono_vocals]))\n",
    "\n",
    "display(Audio(stereo([left, right], fs=fs), rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd7b92",
   "metadata": {},
   "source": [
    "## Frequency plots\n",
    "We wish to visualise the main differences between the two frequency plots of both the original stereo file and the generated spatialised one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_freqs(wav_file, start, end):\n",
    "    fs, data = wavfile.read(wav_file)\n",
    "    data = data[:,0]\n",
    "    plt.figure()\n",
    "    data_to_plot = data[fs*start:fs*end]\n",
    "    plt.plot(data_to_plot)\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Waveform of ' + wav_file)\n",
    "    plt.show()\n",
    "    return data_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dad000",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ster = plot_freqs('ocean_eyes.wav', 30, 31)\n",
    "f_spat = plot_freqs('ocean_eyes_spatialised.wav', 30, 31)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
